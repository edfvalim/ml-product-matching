{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Load the \"autoreload\" extension so that code can change\n",
    "%load_ext autoreload\n",
    "\n",
    "# OPTIONAL: always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "from pathlib import Path\n",
    "import statistics\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from pdb import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "swctest_vanilla = pd.read_json('../../../data/raw/wdc-lspc/gold-standards/computers_new_testset_1500.json.gz', lines=True)\n",
    "swctest = pd.read_pickle('../../../data/interim/wdc-lspc/gold-standards/preprocessed_computers_new_testset_1500.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_column = swctest_vanilla[['pair_id', 'sampling']]\n",
    "swctest_with_sampling = swctest.merge(sampling_column, on='pair_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "swctest_with_sampling['has_training_data'] = False\n",
    "swctest_with_sampling['has_training_data'] = swctest_with_sampling.apply(lambda x: True if x['sampling'] is None or x['sampling'] == 'drop' or x['sampling'] == 'typo' or x['sampling'] == 'hard cases for products in provided training set' else False, axis=1)\n",
    "\n",
    "has_training_data = swctest_with_sampling[swctest_with_sampling['has_training_data'] == True]\n",
    "cluster_ids_with_training_data = set()\n",
    "cluster_ids_with_training_data.update(has_training_data['cluster_id_left'].to_list())\n",
    "cluster_ids_with_training_data.update(has_training_data['cluster_id_right'].to_list())\n",
    "\n",
    "computers_xlarge = pd.read_json('../../../data/raw/wdc-lspc/training-sets/computers_train_xlarge.json.gz', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive pairs per cluster: MEAN=13, MEDIAN=8\n",
      "Negative pairs per cluster: MEAN=158, MEDIAN=148\n",
      "Combined pairs per cluster: MEAN=171, MEDIAN=157\n"
     ]
    }
   ],
   "source": [
    "def get_category_of_training_samples(row):\n",
    "    left_cluster = row['cluster_id_left']\n",
    "    right_cluster = row['cluster_id_right']\n",
    "    try:\n",
    "        if computers_training_lookup_dict[left_cluster]['positives'] > 10 and computers_training_lookup_dict[right_cluster]['positives'] > 10:\n",
    "            return 'many'\n",
    "        elif computers_training_lookup_dict[left_cluster]['positives'] < 5 and computers_training_lookup_dict[right_cluster]['positives'] < 5:\n",
    "            return 'few'\n",
    "        else:\n",
    "            return 'mixed'\n",
    "    except KeyError:\n",
    "        return 'none'\n",
    "\n",
    "computers_training_lookup_dict = dict()\n",
    "positive_count_list = []\n",
    "negative_count_list = []\n",
    "combined_count_list = []\n",
    "\n",
    "for i in cluster_ids_with_training_data:\n",
    "    sub_df = computers_xlarge[(computers_xlarge['cluster_id_left'] == i) | (computers_xlarge['cluster_id_right'] == i)]\n",
    "    value_counts = sub_df['label'].value_counts()\n",
    "    try:\n",
    "        negatives = value_counts[0]\n",
    "    except KeyError:\n",
    "        negatives = 0\n",
    "    try:\n",
    "        positives = value_counts[1]\n",
    "    except KeyError:\n",
    "        positives = 0\n",
    "    if i in computers_training_lookup_dict.keys():\n",
    "        computers_training_lookup_dict[i]['positives'] += positives\n",
    "        computers_training_lookup_dict[i]['negatives'] += negatives\n",
    "    else:\n",
    "        computers_training_lookup_dict[i] = dict()\n",
    "        computers_training_lookup_dict[i]['positives'] = positives\n",
    "        computers_training_lookup_dict[i]['negatives'] = negatives\n",
    "    computers_training_lookup_dict[i]['combined'] = computers_training_lookup_dict[i]['positives'] + computers_training_lookup_dict[i]['negatives']\n",
    "    positive_count_list.append(positives)\n",
    "    negative_count_list.append(negatives)\n",
    "    combined_count_list.append(positives+negatives)\n",
    "    \n",
    "print(f'Positive pairs per cluster: MEAN={statistics.mean(positive_count_list)}, MEDIAN={statistics.median(positive_count_list)}')\n",
    "print(f'Negative pairs per cluster: MEAN={statistics.mean(negative_count_list)}, MEDIAN={statistics.median(negative_count_list)}')\n",
    "print(f'Combined pairs per cluster: MEAN={statistics.mean(combined_count_list)}, MEDIAN={statistics.median(combined_count_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "swctest_with_sampling['amount_training_examples_both'] = swctest_with_sampling.apply(get_category_of_training_samples, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "jointbert_results = pd.read_pickle('../../../src/productbert/saved/models/BT-JointDistilBERT-FT-computers-xlarge-swctest/0829_173424/predictions.pkl.gz')\n",
    "jointbert_results['label_jointbert'] = jointbert_results['predictions'].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "jointbert_results = jointbert_results[['pair_id', 'label_jointbert']]\n",
    "\n",
    "bert_results = pd.read_pickle('../../../src/productbert/saved/models/BT-DistilBERT-FT-computers-xlarge-swctest/0829_172945/predictions.pkl.gz')\n",
    "bert_results['label_bert'] = bert_results['predictions'].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "bert_results = bert_results[['pair_id', 'label_bert']]\n",
    "\n",
    "deepmatcher_results = pd.read_csv('../../../data/processed/inspection/wdc-lspc/deepmatcher/rnn_abs-diff_standard_epochs50_ratio6_batch16_lr0.001_lrdecay0.8_fasttext.en.bin_brand-title_preprocessed_computers_trainonly_xlarge_magellan_pairs_run1_preprocessed_computers_new_testset_1500_magellan_pairs.csv.gz')\n",
    "deepmatcher_results = deepmatcher_results.rename(columns={'label_pred':'label_deepmatcher'})\n",
    "deepmatcher_results = deepmatcher_results[['pair_id', 'label_deepmatcher']]\n",
    "\n",
    "magellan_results = pd.read_pickle('../../../data/processed/inspection/wdc-lspc/magellan/new-testset/preprocessed_computers_train_xlarge_magellan_pairs_formatted_preprocessed_computers_new_testset_1500_magellan_pairs_formatted_RandomForest_brand+title_1.pkl.gz')\n",
    "magellan_results = magellan_results.rename(columns={'pred':'label_magellan'})\n",
    "magellan_results = magellan_results[['pair_id', 'label_magellan']]\n",
    "\n",
    "wordcooc_results = pd.read_pickle('../../../data/processed/inspection/wdc-lspc/wordcooc/new-testset/preprocessed_computers_train_xlarge_wordcooc.pkl.gz_preprocessed_computers_train_xlarge_wordcooc_preprocessed_computers_new_testset_1500.pkl.gz_LogisticRegression_brand+title_1.pkl.gz')\n",
    "wordcooc_results = wordcooc_results.rename(columns={'pred':'label_wordcooc'})\n",
    "wordcooc_results = wordcooc_results[['pair_id', 'label_wordcooc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "swctest_with_sampling_and_labels = swctest_with_sampling.merge(jointbert_results, on='pair_id')\n",
    "swctest_with_sampling_and_labels = swctest_with_sampling_and_labels.merge(bert_results, on='pair_id')\n",
    "swctest_with_sampling_and_labels = swctest_with_sampling_and_labels.merge(deepmatcher_results, on='pair_id')\n",
    "swctest_with_sampling_and_labels = swctest_with_sampling_and_labels.merge(magellan_results, on='pair_id')\n",
    "swctest_with_sampling_and_labels = swctest_with_sampling_and_labels.merge(wordcooc_results, on='pair_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = swctest_with_sampling_and_labels\n",
    "df['challenge_1'], df['challenge_2'], df['challenge_3'], df['challenge_4'], df['challenge_5'], df['challenge_6'], df['challenge_7'] = [0, 0, 0, 0, 0, 0, 0]\n",
    "only_bert = df.loc[((df.label == df.label_bert) & (df.label != df.label_deepmatcher) & (df.label != df.label_magellan))]\n",
    "only_deepmatcher = df.loc[((df.label != df.label_bert) & (df.label == df.label_deepmatcher) & (df.label != df.label_magellan))]\n",
    "only_magellan = df.loc[((df.label != df.label_bert) & (df.label != df.label_deepmatcher) & (df.label == df.label_magellan))]\n",
    "bert_and_deepmatcher = df.loc[((df.label == df.label_bert) & (df.label == df.label_deepmatcher) & (df.label != df.label_magellan))]\n",
    "bert_and_magellan = df.loc[((df.label == df.label_bert) & (df.label != df.label_deepmatcher) & (df.label == df.label_magellan))]\n",
    "deepmatcher_and_magellan = df.loc[((df.label != df.label_bert) & (df.label == df.label_deepmatcher) & (df.label == df.label_magellan))]\n",
    "all_correct = df.loc[((df.label == df.label_bert) & (df.label == df.label_deepmatcher) & (df.label == df.label_magellan))]\n",
    "all_wrong = df.loc[((df.label != df.label_bert) & (df.label != df.label_deepmatcher) & (df.label != df.label_magellan))]\n",
    "\n",
    "######################################################################\n",
    "\n",
    "JOINT_only_joint = df.loc[((df.label == df.label_jointbert) & (df.label != df.label_bert))]\n",
    "JOINT_only_bert = df.loc[((df.label != df.label_jointbert) & (df.label == df.label_bert))]\n",
    "JOINT_both_correct = df.loc[((df.label == df.label_jointbert) & (df.label == df.label_bert))]\n",
    "JOINT_both_wrong = df.loc[((df.label != df.label_jointbert) & (df.label != df.label_bert))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_bert = only_bert[['pair_id','sampling','brand_left','title_left','brand_right','title_right','label','challenge_1','challenge_2','challenge_3','challenge_4','challenge_5','challenge_6','challenge_7','amount_training_examples_both','has_training_data']]\n",
    "only_deepmatcher = only_deepmatcher[['pair_id','sampling','brand_left','title_left','brand_right','title_right','label','challenge_1','challenge_2','challenge_3','challenge_4','challenge_5','challenge_6','challenge_7','amount_training_examples_both','has_training_data']]\n",
    "only_magellan = only_magellan[['pair_id','sampling','brand_left','title_left','brand_right','title_right','label','challenge_1','challenge_2','challenge_3','challenge_4','challenge_5','challenge_6','challenge_7','amount_training_examples_both','has_training_data']]\n",
    "bert_and_deepmatcher = bert_and_deepmatcher[['pair_id','sampling','brand_left','title_left','brand_right','title_right','label','challenge_1','challenge_2','challenge_3','challenge_4','challenge_5','challenge_6','challenge_7','amount_training_examples_both','has_training_data']]\n",
    "bert_and_magellan = bert_and_magellan[['pair_id','sampling','brand_left','title_left','brand_right','title_right','label','challenge_1','challenge_2','challenge_3','challenge_4','challenge_5','challenge_6','challenge_7','amount_training_examples_both','has_training_data']]\n",
    "deepmatcher_and_magellan = deepmatcher_and_magellan[['pair_id','sampling','brand_left','title_left','brand_right','title_right','label','challenge_1','challenge_2','challenge_3','challenge_4','challenge_5','challenge_6','challenge_7','amount_training_examples_both','has_training_data']]\n",
    "all_correct = all_correct[['pair_id','sampling','brand_left','title_left','brand_right','title_right','label','challenge_1','challenge_2','challenge_3','challenge_4','challenge_5','challenge_6','challenge_7','amount_training_examples_both','has_training_data']]\n",
    "all_wrong = all_wrong[['pair_id','sampling','brand_left','title_left','brand_right','title_right','label','challenge_1','challenge_2','challenge_3','challenge_4','challenge_5','challenge_6','challenge_7','amount_training_examples_both','has_training_data']]\n",
    "\n",
    "######################################################################\n",
    "\n",
    "JOINT_only_joint = JOINT_only_joint[['pair_id','sampling','brand_left','title_left','brand_right','title_right','label','challenge_1','challenge_2','challenge_3','challenge_4','challenge_5','challenge_6','challenge_7','amount_training_examples_both','has_training_data']]\n",
    "JOINT_only_bert = JOINT_only_bert[['pair_id','sampling','brand_left','title_left','brand_right','title_right','label','challenge_1','challenge_2','challenge_3','challenge_4','challenge_5','challenge_6','challenge_7','amount_training_examples_both','has_training_data']]\n",
    "JOINT_both_correct = JOINT_both_correct[['pair_id','sampling','brand_left','title_left','brand_right','title_right','label','challenge_1','challenge_2','challenge_3','challenge_4','challenge_5','challenge_6','challenge_7','amount_training_examples_both','has_training_data']]\n",
    "JOINT_both_wrong = JOINT_both_wrong[['pair_id','sampling','brand_left','title_left','brand_right','title_right','label','challenge_1','challenge_2','challenge_3','challenge_4','challenge_5','challenge_6','challenge_7','amount_training_examples_both','has_training_data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only BERT: 71\n",
      "Only Deepmatcher: 75\n",
      "Only Magellan: 27\n",
      "BERT and Deepmatcher: 425\n",
      "BERT and Magellan: 73\n",
      "Deepmatcher and Magellan: 30\n",
      "All correct: 700\n",
      "All wrong: 99\n",
      "#####################################\n",
      "Only Joint: 80\n",
      "Only BERT: 59\n",
      "Both correct: 1210\n",
      "Both wrong: 151\n"
     ]
    }
   ],
   "source": [
    "print(f'Only BERT: {len(only_bert)}')\n",
    "print(f'Only Deepmatcher: {len(only_deepmatcher)}')\n",
    "print(f'Only Magellan: {len(only_magellan)}')\n",
    "print(f'BERT and Deepmatcher: {len(bert_and_deepmatcher)}')\n",
    "print(f'BERT and Magellan: {len(bert_and_magellan)}')\n",
    "print(f'Deepmatcher and Magellan: {len(deepmatcher_and_magellan)}')\n",
    "print(f'All correct: {len(all_correct)}')\n",
    "print(f'All wrong: {len(all_wrong)}')\n",
    "\n",
    "print('#####################################')\n",
    "\n",
    "print(f'Only Joint: {len(JOINT_only_joint)}')\n",
    "print(f'Only BERT: {len(JOINT_only_bert)}')\n",
    "print(f'Both correct: {len(JOINT_both_correct)}')\n",
    "print(f'Both wrong: {len(JOINT_both_wrong)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('../../../data/processed/explain_labeling/').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "only_bert.to_csv('../../../data/processed/explain_labeling/only_bert.csv', index=False)\n",
    "only_deepmatcher.to_csv('../../../data/processed/explain_labeling/only_deepmatcher.csv', index=False)\n",
    "only_magellan.to_csv('../../../data/processed/explain_labeling/only_magellan.csv', index=False)\n",
    "bert_and_deepmatcher.to_csv('../../../data/processed/explain_labeling/bert_and_deepmatcher.csv', index=False)\n",
    "bert_and_magellan.to_csv('../../../data/processed/explain_labeling/bert_and_magellan.csv', index=False)\n",
    "deepmatcher_and_magellan.to_csv('../../../data/processed/explain_labeling/deepmatcher_and_magellan.csv', index=False)\n",
    "all_correct.to_csv('../../../data/processed/explain_labeling/all_correct.csv', index=False)\n",
    "all_wrong.to_csv('../../../data/processed/explain_labeling/all_wrong.csv', index=False)\n",
    "\n",
    "##################################\n",
    "\n",
    "JOINT_only_joint.to_csv('../../../data/processed/explain_labeling/JOINT_only_joint.csv', index=False)\n",
    "JOINT_only_bert.to_csv('../../../data/processed/explain_labeling/JOINT_only_bert.csv', index=False)\n",
    "JOINT_both_correct.to_csv('../../../data/processed/explain_labeling/JOINT_both_correct.csv', index=False)\n",
    "JOINT_both_wrong.to_csv('../../../data/processed/explain_labeling/JOINT_both_wrong.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
