{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.trainer_callback import ProgressCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "\n",
    "class PolishDatasetLoader:\n",
    "    MAIN_DIR_PATH = 'https://github.com/WilyLynx/mlt4pm/raw/master/data/PolishDataset'\n",
    "\n",
    "    @staticmethod\n",
    "    def load_train(type:object, size:object)->pd.DataFrame:\n",
    "        \"\"\"Loads the training dataset from repository\n",
    "\n",
    "        Args:\n",
    "            type (object): dataset type: all, chemia, napoje\n",
    "            size (object): dataset size: small, medium, large\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: training dataset\n",
    "        \"\"\"\n",
    "        path = f'{PolishDatasetLoader.MAIN_DIR_PATH}/{type}_train/pl_wdc_{type}_{size}.json.gz'\n",
    "        df = pd.read_json(path, compression='gzip', lines=True)\n",
    "        return df.reset_index()\n",
    "\n",
    "    @staticmethod\n",
    "    def load_test(type:object)->pd.DataFrame:\n",
    "        \"\"\"Loads the test dataset form repository\n",
    "\n",
    "        Args:\n",
    "            type (object): dataset type: all, chemia, napoje\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: test dataset\n",
    "        \"\"\"\n",
    "        path = f'{PolishDatasetLoader.MAIN_DIR_PATH}/test/pl_wdc_{type}_test.json.gz'\n",
    "        df = pd.read_json(path, compression='gzip', lines=True)\n",
    "        return df.reset_index()\n",
    "\n",
    "\n",
    "class EnglishDatasetLoader:\n",
    "    MAIN_DIR_PATH = 'http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2'\n",
    "\n",
    "    @staticmethod\n",
    "    def load_train(type:object, size:object)->pd.DataFrame:\n",
    "        \"\"\"Loads the training dataset from WDC website\n",
    "\n",
    "        Args:\n",
    "            type (object): dataset type: computers, cameras, watches, shoes, all\n",
    "            size (object): dataset size: small, medium, large, xlarge\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: training dataset\n",
    "        \"\"\"\n",
    "        p = Path(os.path.join('wdc_train', f'{type}_train'))\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "        dataset_path = f'{p}/{type}_train_{size}.json.gz'\n",
    "        if not os.path.exists(dataset_path):\n",
    "            zip_path = f'{p}.zip'\n",
    "            url = f'{EnglishDatasetLoader.MAIN_DIR_PATH}/trainsets/{type}_train.zip'\n",
    "            r = requests.get(url, allow_redirects=True)\n",
    "            open(zip_path, 'wb').write(r.content)\n",
    "            with ZipFile(zip_path, 'r') as zip:\n",
    "                zip.extractall(path=p)\n",
    "        \n",
    "        df = pd.read_json(dataset_path, compression='gzip', lines=True)\n",
    "        return df.reset_index()\n",
    "\n",
    "    @staticmethod\n",
    "    def load_test(type:object)->pd.DataFrame:\n",
    "        \"\"\"Loads the test dataset form repository\n",
    "\n",
    "        Args:\n",
    "            type (object): dataset type: computers, cameras, watches, shoes, all\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: test dataset\n",
    "        \"\"\"\n",
    "        path = f'{EnglishDatasetLoader.MAIN_DIR_PATH}/goldstandards/{type}_gs.json.gz'\n",
    "        df = pd.read_json(path, compression='gzip', lines=True)\n",
    "        return df.reset_index()\n",
    "\n",
    "\n",
    "class FeatureBuilder:\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def get_X(self, dataset):\n",
    "        X = '[CLS] ' + dataset[f'{self.columns[0]}_left']\n",
    "        for i in range(1, len(self.columns)):\n",
    "            X = X + ' [SEP] ' + dataset[f'{self.columns[i]}_left']\n",
    "        for i in range(len(self.columns)):\n",
    "            X = X + ' [SEP] ' + dataset[f'{self.columns[i]}_right']\n",
    "        X + ' [SEP]'\n",
    "        return X.to_list()\n",
    "\n",
    "    def get_y(self, dataset):\n",
    "        return dataset['label'].to_list()\n",
    "\n",
    "\n",
    "class TorchPreprocessedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.items = self.preprocessItems(encodings, labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.items[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def preprocessItems(self, encodings, labels):\n",
    "        items = []\n",
    "        for idx in range(len(labels)):\n",
    "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "            item['labels'] = torch.tensor(self.labels[idx])\n",
    "            items.append(item)\n",
    "        return items\n",
    "\n",
    "\n",
    "def preprocess_train_val(data, feature_builder, tokenizer, split_ratio=0.2):\n",
    "    X_train = feature_builder.get_X(data)\n",
    "    y_train = feature_builder.get_y(data)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=split_ratio, random_state=np.random.RandomState())\n",
    "    train_encodings = tokenizer(X_train, truncation=True, padding=True)\n",
    "    val_encodings = tokenizer(X_val, truncation=True, padding=True)\n",
    "    train_dataset = TorchPreprocessedDataset(train_encodings, y_train)\n",
    "    val_dataset = TorchPreprocessedDataset(val_encodings, y_val)\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "def preprocess_test(data, feature_builder, tokenizer):\n",
    "    X_test = feature_builder.get_X(data)\n",
    "    y_test = feature_builder.get_y(data)\n",
    "    test_encodings = tokenizer(X_test, truncation=True, padding=True)\n",
    "    return TorchPreprocessedDataset(test_encodings, y_test)\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "\n",
    "def train_model(model_name, dataset_type, dataset_size, lang, google=False):\n",
    "    print(f'BEGIN  EXPERIMENT')\n",
    "    print(f'model: {model_name}')\n",
    "    print(f'dataset: {dataset_type}')\n",
    "    print(f'size: {dataset_size}')\n",
    "    if google:\n",
    "        drive.mount('/content/drive')\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    title_fb = FeatureBuilder(['title'])\n",
    "\n",
    "    if lang == 'PL':\n",
    "        dataset_loader = PolishDatasetLoader\n",
    "    elif lang == 'ENG':\n",
    "        dataset_loader = EnglishDatasetLoader\n",
    "\n",
    "    train_df = dataset_loader.load_train(dataset_type, dataset_size)\n",
    "    train_dataset, val_dataset = preprocess_train_val(train_df, title_fb, tokenizer)\n",
    "\n",
    "    test_df = dataset_loader.load_test(dataset_type)\n",
    "    test_dataset = preprocess_test(test_df, title_fb, tokenizer)\n",
    "\n",
    "    logdir_name = f'{model_name}_{dataset_type}_{dataset_size}'\n",
    "    logdir = os.path.join(\"logs\", logdir_name)\n",
    "    train_batch_size = 16\n",
    "    num_train_epochs = 10\n",
    "    half_train = (len(train_dataset) * num_train_epochs) // (2*train_batch_size)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',          \n",
    "        num_train_epochs=num_train_epochs,              # total number of training epochs\n",
    "        per_device_train_batch_size=train_batch_size,   # batch size per device during training\n",
    "        per_device_eval_batch_size=64,    # batch size for evaluation\n",
    "        warmup_steps=half_train,                 # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,                # strength of weight decay\n",
    "        logging_dir=logdir,               # directory for storing logs\n",
    "        logging_steps=10,                 # for training metrics\n",
    "        disable_tqdm=False,               # show some progress\n",
    "        fp16=True,                        # float 16 acceleration\n",
    "        evaluation_strategy='epoch',      # evaluate after epoch\n",
    "        load_best_model_at_end =True,     # load best model\n",
    "        metric_for_best_model='eval_f1',  # use model with best F1 score\n",
    "        save_total_limit=5,               # store last 5 checkpoints  ,\n",
    "        seed=np.random.randint(1_000_000) # random seed\n",
    "    )\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    print('DEVICE USED: ', training_args.device)\n",
    "    print('TRAINING')\n",
    "    trainer.train()\n",
    "    print('EVALUATION ON VALIDATION SET')\n",
    "    eval = trainer.evaluate()\n",
    "    print(eval)\n",
    "\n",
    "    print('TESTING')\n",
    "    pred = trainer.predict(test_dataset)\n",
    "    metrics = pd.DataFrame(compute_metrics(pred), index=[0])\n",
    "    metrics['model'] = model_name\n",
    "    metrics['dataset_type'] = dataset_type\n",
    "    metrics['dataset_size'] = dataset_size\n",
    "    with pd.option_context('display.max_columns', None):\n",
    "        print(metrics)\n",
    "\n",
    "    print('SAVING MODEL')\n",
    "    model_tmp_save = 'results/test'\n",
    "    model.save_pretrained(model_tmp_save)\n",
    "    if google:\n",
    "        DRIVE = 'drive/MyDrive'\n",
    "        p = Path(os.path.join(DRIVE, 'MLT4PM', lang, model_name, dataset_type, dataset_size))\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "        os.system(f'python -m transformers.convert_graph_to_onnx --model {model_tmp_save} --framework pt --tokenizer {model_name} {p}/model/model.onnx')\n",
    "        os.system(f'mv {p}/model/model.onnx {p}/model.onnx')\n",
    "        os.system(f'rm -R {p}/model/')\n",
    "        os.system(f'rm -R {model_tmp_save}')\n",
    "        metrics.to_csv(f'{p}/metrics.csv')\n",
    "\n",
    "        log_path = Path(os.path.join(DRIVE, \"MLT4PM\", lang, \"logs\", logdir_name))\n",
    "        log_path.mkdir(parents=True, exist_ok=True)\n",
    "        os.system(f'cp -R {logdir} {log_path}')\n",
    "    os.system('rm -R ./results')\n",
    "    print('EXPERIMENT ENDED \\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-multilingual-uncased'\n",
    "lang = 'PL'\n",
    "for d in ['chemia', 'napoje', 'all']:\n",
    "    for s in ['small', 'medium']:\n",
    "      for i in range(3):\n",
    "        print(f'Experiment {i+1} {d} {s}')\n",
    "        train_model(model_name, d, s, lang, google=False)"
   ]
  }
 ]
}